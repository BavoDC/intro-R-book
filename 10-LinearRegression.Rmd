# Linear Regression Models in R {#lms}

Your journey as a model builder in R will start from studying linear models and the use of the `lm` function.

## A simple linear regression model

You analyze Ford dealership data as registered in Milwaukee, September/October 1990. Data on 62 credit card applicants are available, including the car purchase price $Y$ and the applicant's annual income $X$. Data are available in the `.csv` file `car_price`.

You first load the data.
```{r}
path <- file.path('data')
path.car <- file.path(path, "car_price.csv")
car_price <- read.csv(path.car)
```
Then you explore the data.
```{r}
attach(car_price)
summary(price)
summary(income)

# average
mean(price)
mean(income)

# standard deviation
sd(price)
sd(income)

# the 5-th and 95-th percentiles
quantile(price, c(0.05, 0.95))
quantile(income, c(0.05, 0.95))

# histograms of price and income
# density histogram for 'price'
hist(price, br = 20, xlim = c(5000, 30000), col="grey", freq=FALSE)
lines(density(price), col=4)
# frequency histogram for 'income/1000'
hist(income/1000, br=10, xlab="income (in $000's)", xlim=c(0, 120), col="grey")

# scatter plot 'income/1000' versus 'price'
plot(income/1000, price, pch=21, cex=1.2, xlab="income (in $000's)")
detach(car_price)
```
Explore the data with `ggplot`.
```{r}
library("ggplot2")
ggplot(car_price, aes(x = income/1000, y = price)) +
  theme_bw() +
  geom_point(shape=1, alpha = 1/2) + 
  geom_smooth() 
```

You will now fit a simple regression model with income as predictor to purchase price. That is:

\begin{eqnarray*}
Y_i &=& \beta_0+\beta_1 \cdot x_i +\epsilon_i,
\end{eqnarray*}
where $Y_i$ is the car price for observation $i$, $x_i$ the corresponding income and $\epsilon_i$ an error term. $\beta_0$ is the intercept and $\beta_1$ the slope.


Recall that fitting a (simple) linear regression model implies minimizing the residual sum of squares. That is:

\begin{eqnarray*}
(\hat{\beta}_0,\ \hat{\beta}_1) = \min_{\beta_0, \beta_1} \sum_{i=1}^n \left(Y_i - (\beta_0+\beta_1 \cdot x_{i})\right)^2,
\end{eqnarray*}
and the fitted values are then specified as $\hat{y}_i=\hat{\beta}_0+\hat{\beta}_1\cdot x_{i}$. The corresponding residuals are then defined as $\hat{\epsilon}_i = y_i - \hat{y}_i$. 

You assign the output of the `lm` function to the object `lm1`.
```{r}
lm1 <- lm(price ~ income, data = car_price)
summary(lm1)
# check attributes of object 'lm1'
names(lm1)
# some useful stuff: 'coefficients', 'residuals', 'fitted.values', 'model'
lm1$coef
lm1$residuals
lm1$fitted.values
```

To visualize this linear model fit you can use the built-in `plot` function, applied to object `lm1`.
```{r}
# use built-in plot function
# you may have noticed that we have used the function plot with all kinds of arguments: 
# one or two variables, a data frame, and now a linear model fit;
# in R jargon plot is a generic function; it checks for the kind of object that you # are plotting and then calls the appropriate (more specialized) function to do the work.
plot(lm1)
```

Or you can construct your own plots, e.g. by adding the least squares line to the scatter plot.
```{r}
# add the regression line to the scatter plot
plot(car_price$income, car_price$price, pch=21, cex=1.2, xlab = "income", main = "Simple linear regression")
# add LS line like this
abline(lm1, col="blue", lwd=2)
# or like this
abline(lm1$coefficients[1], lm1$coefficients[2])
```

Similarly, you can illustrate the fit with `ggplot`.
```{r}
ggplot(car_price, aes(x = income, y = price)) + 
 theme_bw() +
 geom_point(shape=1, alpha = 1/2) + 
 geom_smooth()+geom_abline(intercept = lm1$coef[1], slope = lm1$coef[2],   colour="red", size=1.25) 
```

The least squares fit minimizes the sum of the squares of the vertical distances between the observed response values and the least squares line (or plane). You now graphically illustrate what vertical distance means.
```{r}
plot(car_price$income, car_price$price, pch=21, cex=1.2, xlab = "income", main = "Simple linear regression")
abline(lm1, col = "blue", lwd=2)
segments(car_price$income, car_price$price, car_price$income, lm1$fitted.values, lty=1)
```

You now return to the `summary(lm1)` and try to understand the (rest of the) output that is printed. 
```{r}
summary(lm1)
```
Recall that in a general linear model $Y = X\beta + \epsilon$ where $E[\epsilon]=0$ and $\text{Var}(\epsilon)=\sigma^2 I$ with $I$ the identity matrix, the following estimator is used for the variance of the error terms
\begin{eqnarray*}
s^2 &=& \frac{1}{n-(p+1)}(Y-X\hat{\beta})^{'}(Y-X\hat{\beta}),
\end{eqnarray*}
where $\beta = (\beta_0, \beta_1, \ldots, \beta_p)^{'}$.

You can recognize this in the output of `lm1` as follows
```{r}
error.SS <- sum(lm1$resid^2)
error.SS
sqrt(error.SS/(nrow(car_price)-2))
```

The proportion of the variability in the data that is explained by the regression model is
\begin{eqnarray*}
R^2 &=& \frac{\text{Regression SS}}{\text{Total SS}} \\
&=& \frac{\sum_{i=1}^n (\hat{Y}_i-\bar{Y})^2}{\sum_{i=1}^n (Y_i-\bar{Y})^2} \\
&=& \frac{\sum_{i=1}^n (Y_i-\bar{Y})^2 - \sum_{i=1}^n (Y_i-\hat{Y})^2}{\sum_{i=1}^n (Y_i-\bar{Y})^2}.
\end{eqnarray*}

```{r}
attach(car_price)
total.SS <- sum((price-mean(price))^2)
total.SS
error.SS <- sum(lm1$resid^2)
error.SS

# R^2?
(total.SS-error.SS)/total.SS
detach(car_price)
```

Finally, the output of `lm1` displays the result of a so-called $F$ test, constructed as follows:
```{r}
attach(car_price)
# anova table in R?
anova(lm1)

# F-statistic in anova and in output lm1?
lm0 <- lm(price ~ 1)
error0.SS <- sum(lm0$resid^2)

# calculate F-statistic
F <- ((anova(lm0)$"Sum Sq")-(anova(lm1)$"Sum Sq"[2]))/(anova(lm1)$"Mean Sq"[2]) 
F
# critical values
qf(0.95, 1, 60)
1-pf(F, 1, 60)

detach(car_price)
```
---

## A multiple linear regression model

You'll now move on from simple to multiple linear regression. You model the 
data by McDonald and Schwing (1973) published in Technometrics. The sampled data consists of variables obtained for year 1960 for 60 Standard Metropolitan
Statistical Areas (SMSA) in the US. The goal is to relate mortality in these SMSA to explanatory variables. For each sample area, you have information concerning the age-adjusted mortality rate for all causes, expressed as deaths per 100,000 population. This will be your response variable. The list of explanatory variables is:

* weather-related variables:
 + `prec`: average annual precipitation in inches
 + `jant`: average January temperature in degrees F
 + `jult`: average July temperature in degrees F
 + `humid`: annual average % relative humidity at 1 pm
* scocio-economic characteristics:
 + `ovr65`: % of 1960 SMSA population aged 65 and older
 + `popn`: average household size
 + `educ` : median school years completed by those over 22
 + `hous` : % of housing units which are sound and with all facilities
 + `educ` : median school years completed by those over 22
 + `dens` : population per sq mile in urbanized areas, 1960
 + `nonw` : % of non-white population in urbanized areas, 1960
 + `wwdrk` : % of employed in white collar occupations
 + `poor` : % of families with income less than $3,000
* pollutants:
 + `hc` : relative pollution potential of hydrocarbons
 + `nox` : relative pollution potential of oxides of nitrogen
 + `so2`: relative pollution potential of sulfur dioxides

First, you'll load the data.
```{r}
path <- file.path('data')
path.mort <- file.path(path, "pollution.csv")
mort_poll <- read.csv(path.mort)
```
Then, you'll explore the data.
```{r}
attach(mort_poll)
summary(mort_poll)
# get correlation matrix
round(cor(mort_poll), 4)

# create dataframes
# weather related vars
mort_poll_1 <- data.frame(mort, prec, jant, jult, humid)
# socio-economic vars
mort_poll_2 <- data.frame(mort, ovr65, popn, educ, hous, dens, nonw, wwdrk, poor)
# pollution effects
mort_poll_3 <- data.frame(mort, hc, nox, so2)

# matrix scatterplots
pairs(mort_poll_1, cex=1, pch=19)
pairs(mort_poll_2, cex=0.5, pch=19)
pairs(mort_poll_3, cex=1, pch=19)
detach(mort_poll)
```

First, you fit a rather simple linear model to explain `mort`. That is 
\begin{eqnarray*}
Y &=& \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \epsilon_i,
\end{eqnarray*}
where $\beta_0$ is the intercept, $x_1$ the `educ` and $x_2$ the `so2`.
```{r}
attach(mort_poll)
lm1 <- lm(mort ~ educ + so2)
summary(lm1)
detach(mort_poll)
```

You inspect the analysis-of-variance table for this linear model `lm1`. That is
```{r}
anova(lm1)
attach(mort_poll)
lm0 <- lm(mort ~ 1)
lm_educ <- lm(mort ~ educ)
anova(lm_educ)
F_educ <- ((anova(lm0)$"Sum Sq")-(anova(lm_educ)$"Sum Sq"[2]))/(anova(lm1)$"Mean Sq"[3]) 
F_educ
F_so2 <- ((anova(lm_educ)$"Sum Sq"[2])-(anova(lm1)$"Sum Sq"[3]))/(anova(lm1)$"Mean Sq"[3]) 
F_so2
detach(mort_poll)
```


You will now use the object `lm1` to construct confidence and prediction intervals for a single observation. Given a set of predictor values in $x_0$ the predicted response is $\hat{y}_0 = x_0^{'}\hat{\beta}$. The uncertainty in predicting the mean response is $\text{Var}(x_0^{'}\hat{\beta})$ whereas the uncertainty in predicting the value of an observation is $\text{Var}(x_0^{'}\hat{\beta}+\epsilon_0)$.
```{r}
attach(mort_poll)
x0 <- data.frame(educ = 10, so2 = exp(2))
predict(lm1, x0, interval = "confidence")
predict(lm1, x0, interval = "prediction")
detach(mort_poll)
```
For a grid of `educ` values, when `so2` is fixed, this goes as follows:
```{r}
attach(mort_poll)
grid <- seq(8, 15, 0.1)
x.new <- data.frame(educ = grid, so2 = exp(2))
p <- predict(lm1, x.new, se=TRUE, interval="prediction")
p1 <- predict(lm1, x.new, se=TRUE, interval="confidence")
# use `matplot` to plot the columns of one matrix against the columnsof another
matplot(grid, p$fit, lty=c(1,2,2), col=c("black", "red", "red"), type = "l", xlab = "educ", ylab = "mort", main = "Predicted mort over a range of educ, log(so2)=2")
matlines(grid, p1$fit, lty = c(1, 2, 2), col = c("black", "blue", "blue"))
rug(educ)
# for an explanation wrt different shapes, see 
# http://stats.stackexchange.com/questions/85560/shape-of-confidence-interval-for-p# redicted-values-in-linear-regression
detach(mort_poll)
```

Then you fit a linear model with all 15 variables in the dataset.
```{r}
attach(mort_poll)
lm2 <- lm(mort ~ prec + jant + jult + humid + hc + nox + so2 + ovr65 + popn + educ + hous + dens + nonw + wwdrk + poor)
lm2$coef
detach(mort_poll)
```

Now perform model selection stepwise, based on AIC. 
```{r}
# model selection based on AIC
library(MASS)
attach(mort_poll)
lm1 <- lm(mort ~ 1)
# get AIC, mind the difference
AIC(lm1)
extractAIC(lm1)
# for linear models with unknown scale (i.e., for lm and aov), 
# -2 log L is computed from the deviance and uses a different additive constant to 
# logLik and hence AIC 

# forward search
stepAIC(lm1, list(upper = ~ prec + jant + jult + ovr65 + popn + educ + hous + dens + nonw + wwdrk + poor + hc + log(nox) + log(so2) + humid, lower = ~ 1), direction = "forward")

# backward search
lm1 <- lm(mort ~ prec + jant + jult + ovr65 + popn + educ + hous + dens + nonw +
		 wwdrk + poor + hc + log(nox) + log(so2) + humid)
stepAIC(lm1, list(upper = ~ prec + jant + jult + ovr65 + popn + educ + hous + dens + nonw + wwdrk + poor + hc + log(nox) + log(so2) + humid, lower = ~ 1), direction = "backward")

# both directions search
lm1 <- lm(mort ~ 1)
lm1 <- lm(mort ~ prec + jant + jult + ovr65 + popn + educ + hous + dens + nonw +
		 wwdrk + poor + hc + log(nox) + log(so2) + humid)
stepAIC(lm1, list(upper = ~ prec + jant + jult + ovr65 + popn + educ + hous + dens + nonw + wwdrk + poor + hc + log(nox) + log(so2) + humid, lower = ~ 1), direction = "both")
detach(mort_poll)
```
--- 

## Exercises

```{block linear_models, purl=FALSE}
**_Learning check_**
```

1. Load the Boston Housing dataset from the `mlbench` package. 
+ Use the following instructions
```{r}
library(mlbench)
data("BostonHousing")
```
+ Inspect the different types of variables present.
+ Explore and visualize the distribution of our target variable `medv`.
+ Explore and visualize any potential correlations between `medv` and the variables `crim`, `rm`, `age`, `rad`, `tax` and `lstat`.
+ Set a seed of 123 and split your data into a train and test set using a 75/25 split. You may find the `caret` library helpful here.
+ We have seen that `crim`, `rm`, `tax`, and `lstat` could be good predictors of `medv`. To get the ball rolling, let us fit a linear model for these terms.
+ Obtain an R-squared value for your model and examine the diagnostic plots found by plotting your linear model.
+ We can see a few problems with our model immediately with variables such as 381 exhibiting a high leverage, a poor QQ plot in the tails a relatively poor r-squared value. Let us try another model, this time transforming `medv` due to the positive skewness it exhibited.
+ Examine the diagnostics for the model. What do you conclude? Is this an improvement on the first model?
 One assumption of a linear model is that the mean of the residuals is zero. You could try and test this.
+ Create a data frame of your predicted values and the original values.
+ Plot this to visualize the performance of your model.

```{block, purl=FALSE}
```
